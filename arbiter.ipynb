{"cells":[{"cell_type":"markdown","metadata":{"id":"WznAE_5joZwV"},"source":["# README\n","\n","Welcome!\n","\n","In this script, we implement hyper-learning to perform gradient-based adaptation of the batch size in line with our paper (...). \n","\n","The notebook provides a walkthrough style format that has been written to be as plug-and-play as possible - you are mostly likely interested in running experiments after all! \n","\n","For this reason, the only thing you'll need to do to get the experiments underway is to change the root_path and save directories to match your own Google Drive, and that's it! Simply head to the 'run' section to create the directories and start training. \n","\n","If you are also interested in learning more about the various components of the code (e.g. architecture for Arbiter, optimisation routines, linear mapping to feature space, etc), you'll find descriptions at each respective section.\n","\n","Happy experimenting!\n","\n","(p.s. please get in touch at ... if there are any issues with the code)."]},{"cell_type":"markdown","metadata":{"id":"884Ed4bH75ib"},"source":["# Import g-drive"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2796,"status":"ok","timestamp":1660132124282,"user":{"displayName":"Calum MacLellan","userId":"18398237058780708126"},"user_tz":-60},"id":"DxAEmVYJ7uKD","outputId":"e095d790-cd57-4579-c0cf-e17cb07708e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","root_path  = 'gdrive/My Drive/ho/hyperlearning/batch-size'\n"]},{"cell_type":"markdown","metadata":{"id":"nGs3GgcMVA5B"},"source":["GPU info"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1660132124283,"user":{"displayName":"Calum MacLellan","userId":"18398237058780708126"},"user_tz":-60},"id":"yPIlyBe0U_-i","outputId":"6d42ffcc-571a-463e-9fa4-fc674bba9ab9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Aug 10 11:48:43 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"markdown","metadata":{"id":"1l-MNVnv7qrb"},"source":["# dataset.py"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"J_k2Es7I6Bu5","executionInfo":{"status":"ok","timestamp":1660128438687,"user_tz":-60,"elapsed":3444,"user":{"displayName":"Calum MacLellan","userId":"18398237058780708126"}}},"outputs":[],"source":["\n","import os \n","import numpy as np\n","import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","data_loc = '/data'\n","\n","def get_dataloaders(dataset, batch_size, train_percent=0.8):\n","  if dataset == 'mnist':\n","      return mnist_data_loader(batch_size, train_percent)\n","  elif dataset == 'svhn':\n","      return svhn_data_loader(batch_size, train_percent)\n","  elif dataset == 'cifar_10':\n","      return cifar_10_data_loader(batch_size, train_percent)\n","  elif dataset == 'cifar_100':\n","      return cifar_100_data_loader(batch_size, train_percent)\n","  else:\n","      raise Exception('dataset is not supported')\n","\n","def mnist_data_loader(batch_size, train_percent):\n","  transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n","  \n","  ''' datasets\n","  '''\n","  train_dataset = datasets.MNIST(root=data_loc, train=True, download=True, transform=transform)\n","  test_dataset = datasets.MNIST(root=data_loc, train=False, transform=transform, )\n","  \n","  ''' dataloaders \n","  '''\n","  num_train = len(train_dataset)\n","  indices = list(range(num_train))\n","  split = int(np.floor(train_percent * num_train))\n","  # training\n","  train_loader = DataLoader(\n","    train_dataset, batch_size=batch_size,\n","    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n","    pin_memory=False, num_workers=1) \n","  # validation data\n","  val_loader = DataLoader(\n","    train_dataset, batch_size=batch_size,\n","    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n","    pin_memory=False, num_workers=1)\n","  # test data \n","  test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size, num_workers=1)\n","\n","  custom_loader = CustomDataLoader(train_dataset, transform, train_percent)\n","\n","  return {'train':train_loader, 'val':val_loader, 'test':test_loader, 'custom':custom_loader}\n","  \n","def svhn_data_loader(batch_size, train_percent):\n","  transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n","  \n","  ''' datasets\n","  '''\n","  train_dataset = datasets.SVHN(root=data_loc, split='train', download=True, transform=transform)\n","  test_dataset = datasets.SVHN(root=data_loc, split='test', transform=transform)\n","\n","  ''' dataloaders \n","  '''\n","  num_train = len(train_dataset)\n","  indices = list(range(num_train))\n","  split = int(np.floor(train_percent * num_train))\n","  # training\n","  train_loader = DataLoader(\n","    train_dataset, batch_size=batch_size,\n","    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n","    pin_memory=False, num_workers=1) \n","  # validation data\n","  val_loader = DataLoader(\n","    train_dataset, batch_size=batch_size,\n","    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n","    pin_memory=False, num_workers=1)\n","  # test data \n","  test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size, num_workers=1)\n","\n","  custom_loader = CustomDataLoader(train_dataset, transform, train_percent)\n","\n","  return {'train':train_loader, 'val':val_loader, 'test':test_loader, 'custom':custom_loader}\n","  \n","def cifar_10_data_loader(batch_size, train_percent):\n","  # transforms\n","  transform_train = transforms.Compose(\n","      [transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), ])\n","  transform_test = transforms.Compose([transforms.ToTensor(), transforms.Normalize(\n","      (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), ])\n","  \n","  # datasets\n","  train_dataset = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=transform_train)\n","  test_dataset = datasets.CIFAR10(root=data_loc, train=False, transform=transform_test)\n","  \n","  # dataloaders with validation/hessian subsets of the training dataset\n","  num_train = len(train_dataset)\n","  indices = list(range(num_train))\n","  split = int(np.floor(train_percent * num_train))\n","\n","  train_loader = DataLoader(\n","    train_dataset, batch_size=batch_size,\n","    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n","    pin_memory=False, num_workers=1) \n","\n","  # validation data\n","  val_loader = DataLoader(\n","    train_dataset, batch_size=batch_size,\n","    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n","    pin_memory=False, num_workers=1)\n","\n","  # test data \n","  test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size, num_workers=1)\n","\n","  custom_loader = CustomDataLoader(train_dataset, transform_train, train_percent)\n","\n","  return {'train':train_loader, 'val':val_loader, 'test':test_loader, 'custom':custom_loader}\n","  \n","\n","def cifar_100_data_loader(batch_size, train_percent):\n","  transform_train = transforms.Compose(\n","      [transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)), ])\n","  transform_test = transforms.Compose([transforms.ToTensor(), transforms.Normalize(\n","      (0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)), ])\n","  \n","  '''datasets '''\n","  train_dataset = datasets.CIFAR100(root=data_loc, train=True, download=True, transform=transform_train)\n","  test_dataset = datasets.CIFAR100(root=data_loc, train=False, transform=transform_test)\n","\n","  ''' dataloaders '''\n","  num_train = len(train_dataset)\n","  indices = list(range(num_train))\n","  split = int(np.floor(train_percent * num_train))\n","\n","  train_loader = DataLoader(\n","    train_dataset, batch_size=batch_size,\n","    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n","    pin_memory=True, num_workers=1) \n","\n","  # validation data\n","  val_loader = DataLoader(\n","    train_dataset, batch_size=batch_size,\n","    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n","    pin_memory=True, num_workers=1)\n","\n","  # test data \n","  test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size, num_workers=1)\n","  \n","  custom_loader = CustomDataLoader(train_dataset, transform_train, train_percent)\n","\n","  return {'train':train_loader, 'val':val_loader, 'test':test_loader, 'custom':custom_loader}\n","  "]},{"cell_type":"markdown","metadata":{"id":"iZdeOoNP8Mx7"},"source":["# custom_dataloader.py"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"HeA_1h_g8OnA","executionInfo":{"status":"ok","timestamp":1660128438688,"user_tz":-60,"elapsed":6,"user":{"displayName":"Calum MacLellan","userId":"18398237058780708126"}}},"outputs":[],"source":["\n","\n","'''\n","@started: 17/11/21\n","@author: calmac\n","'''\n","\n","import os \n","import numpy as np\n","from PIL import Image\n","import torch\n","from torchvision import datasets, transforms\n","from torch.utils import data \n","\n","class CustomDataLoader(object):\n","  \n","    def __init__(self, dataset, transform, split):\n","\n","        self.num_train = len(dataset)\n","        self.val_start = int(np.floor(split * self.num_train))\n","        self.data = dataset.data\n","        self.targets = dataset.targets\n","        self.transform = transform\n","\n","    def __call__(self, sample_size):\n","\n","        \n","        idx = list(np.random.randint(low=self.val_start, high=self.num_train, size=sample_size))\n","        \n","        X = np.array([self.data[i] for i in idx])\n","        labels = torch.from_numpy(np.array([self.targets[i] for i in idx]))\n","\n","        for i in range(sample_size):\n","          xi = Image.fromarray(X[i,:])\n","          xi = self.transform(xi).unsqueeze(0)\n","          if i==0:\n","            inputs = xi\n","          else:\n","            inputs = torch.cat((inputs,xi), dim=0)\n","\n","        return inputs, labels\n"]},{"cell_type":"markdown","metadata":{"id":"h9FtSgPLRwx7"},"source":["# hd.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wbf91Q60RzXZ"},"outputs":[],"source":["import math\n","import torch\n","from torch.optim.optimizer import Optimizer,required\n","from functools import reduce\n","\n","\n","class SGDHD(Optimizer):\n","    r\"\"\"Implements stochastic gradient descent (optionally with momentum).\n","    Nesterov momentum is based on the formula from\n","    `On the importance of initialization and momentum in deep learning`__.\n","    Args:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float): learning rate\n","        momentum (float, optional): momentum factor (default: 0)\n","        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n","        dampening (float, optional): dampening for momentum (default: 0)\n","        nesterov (bool, optional): enables Nesterov momentum (default: False)\n","        hypergrad_lr (float, optional): hypergradient learning rate for the online\n","        tuning of the learning rate, introduced in the paper\n","        `Online Learning Rate Adaptation with Hypergradient Descent`_\n","    Example:\n","        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","        >>> optimizer.zero_grad()\n","        >>> loss_fn(model(input), target).backward()\n","        >>> optimizer.step()\n","    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n","    .. _Online Learning Rate Adaptation with Hypergradient Descent:\n","        https://openreview.net/forum?id=BkrsAzWAb\n","    .. note::\n","        The implementation of SGD with Momentum/Nesterov subtly differs from\n","        Sutskever et. al. and implementations in some other frameworks.\n","        Considering the specific case of Momentum, the update can be written as\n","        .. math::\n","                  v = \\rho * v + g \\\\\n","                  p = p - lr * v\n","        where p, g, v and :math:`\\rho` denote the parameters, gradient,\n","        velocity, and momentum respectively.\n","        This is in contrast to Sutskever et. al. and\n","        other frameworks which employ an update of the form\n","        .. math::\n","             v = \\rho * v + lr * g \\\\\n","             p = p - v\n","        The Nesterov version is analogously modified.\n","    \"\"\"\n","\n","    def __init__(self, params, lr=required, momentum=0, dampening=0,\n","                 weight_decay=0, nesterov=False, hypergrad_lr=0, results_files=None):\n","        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n","                        weight_decay=weight_decay, nesterov=nesterov, hypergrad_lr=hypergrad_lr)\n","        if nesterov and (momentum <= 0 or dampening != 0):\n","            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n","        super(SGDHD, self).__init__(params, defaults)\n","\n","        if len(self.param_groups) != 1:\n","            raise ValueError(\"SGDHD doesn't support per-parameter options (parameter groups)\")\n","\n","        self._params = self.param_groups[0]['params']\n","        self._params_numel = reduce(lambda total, p: total + p.numel(), self._params, 0)\n","\n","        self.results_files = results_files\n","\n","    def _gather_flat_grad_with_weight_decay(self, weight_decay=0):\n","        views = []\n","        for p in self._params:\n","            if p.grad is None:\n","                view = torch.zeros_like(p.data)\n","            elif p.grad.data.is_sparse:\n","                view = p.grad.data.to_dense().view(-1)\n","            else:\n","                view = p.grad.data.view(-1)\n","            if weight_decay != 0:\n","                view.add_(weight_decay, p.data.view(-1))\n","            views.append(view)\n","        return torch.cat(views, 0)\n","\n","    def _add_grad(self, step_size, update):\n","        offset = 0\n","        for p in self._params:\n","            numel = p.numel()\n","            # view as to avoid deprecated pointwise semantics\n","            p.data.add_(step_size, update[offset:offset + numel].view_as(p.data))\n","            offset += numel\n","        assert offset == self._params_numel\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        assert len(self.param_groups) == 1\n","\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        group = self.param_groups[0]\n","        weight_decay = group['weight_decay']\n","        momentum = group['momentum']\n","        dampening = group['dampening']\n","        nesterov = group['nesterov']\n","\n","        grad = self._gather_flat_grad_with_weight_decay(weight_decay)\n","\n","        # NOTE: SGDHD has only global state, but we register it as state for\n","        # the first param, because this helps with casting in load_state_dict\n","        state = self.state[self._params[0]]\n","        # State initialization\n","        if len(state) == 0:\n","            state['grad_prev'] = torch.zeros_like(grad)\n","\n","        grad_prev = state['grad_prev']\n","        # Hypergradient for SGD\n","        h = torch.dot(grad, grad_prev)\n","        # Hypergradient descent of the learning rate:\n","        group['lr'] += group['hypergrad_lr'] * h\n","        write_results('{}'.format(h.item()), self.results_files['h'])\n","        write_results('{}'.format(group['lr'].data), self.results_files['lrit'])\n","\n","        if momentum != 0:\n","            if 'momentum_buffer' not in state:\n","                buf = state['momentum_buffer'] = torch.zeros_like(grad)\n","                buf.mul_(momentum).add_(grad)\n","            else:\n","                buf = state['momentum_buffer']\n","                buf.mul_(momentum).add_(1 - dampening, grad)\n","            if nesterov:\n","                grad.add_(momentum, buf)\n","            else:\n","                grad = buf\n","\n","        state['grad_prev'] = grad\n","\n","        self._add_grad(-group['lr'], grad)\n","\n","        return loss\n","\n","\n","class AdamHD(Optimizer):\n","    \"\"\"Implements Adam algorithm.\n","\n","    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n","\n","    Arguments:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float, optional): learning rate (default: 1e-3)\n","        betas (Tuple[float, float], optional): coefficients used for computing\n","            running averages of gradient and its square (default: (0.9, 0.999))\n","        eps (float, optional): term added to the denominator to improve\n","            numerical stability (default: 1e-8)\n","        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n","        hypergrad_lr (float, optional): hypergradient learning rate for the online\n","        tuning of the learning rate, introduced in the paper\n","        `Online Learning Rate Adaptation with Hypergradient Descent`_\n","\n","    .. _Adam\\: A Method for Stochastic Optimization:\n","        https://arxiv.org/abs/1412.6980\n","    .. _Online Learning Rate Adaptation with Hypergradient Descent:\n","        https://openreview.net/forum?id=BkrsAzWAb\n","    \"\"\"\n","\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=0, hypergrad_lr=1e-8, results_files=None):\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, hypergrad_lr=hypergrad_lr)\n","        super(AdamHD, self).__init__(params, defaults)\n","        \n","        self.results_files = results_files\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n","\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","\n","                if group['weight_decay'] != 0:\n","                    grad = grad.add(group['weight_decay'], p.data)\n","\n","                if state['step'] > 1:\n","                    prev_bias_correction1 = 1 - beta1 ** (state['step'] - 1)\n","                    prev_bias_correction2 = 1 - beta2 ** (state['step'] - 1)\n","                    # Hypergradient for Adam:\n","                    h = torch.dot(grad.view(-1), torch.div(exp_avg, exp_avg_sq.sqrt().add_(group['eps'])).view(-1)) * math.sqrt(prev_bias_correction2) / prev_bias_correction1\n","                    # Hypergradient descent of the learning rate:\n","                    group['lr'] += group['hypergrad_lr'] * h\n","                    write_results('{}'.format(h.item()), self.results_files['h'])\n","                    write_results('{}'.format(group['lr'].data), self.results_files['lrit'])\n","                \n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                denom = exp_avg_sq.sqrt().add_(group['eps'])\n","\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n","\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","        return loss"]},{"cell_type":"markdown","metadata":{"id":"zUgoj-3L8QC7"},"source":["# network.py"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"OgRAdcLa8iaJ","executionInfo":{"status":"ok","timestamp":1660128440954,"user_tz":-60,"elapsed":474,"user":{"displayName":"Calum MacLellan","userId":"18398237058780708126"}}},"outputs":[],"source":["\n","\n","'''\n","@author: calmac\n","@started: 13/10/21\n","\n","'''\n","\n","import math\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.init as init\n","\n","def get_network(network_name, dataset):\n","    if dataset == 'mnist':\n","        return network(network_name, num_classes=10, num_channels=1)\n","    elif dataset == 'svhn':\n","        return network(network_name, num_classes=10, num_channels=3)\n","    elif dataset == 'cifar_10':\n","        return network(network_name, num_classes=10, num_channels=3)\n","    elif dataset == 'cifar_100':\n","        return network(network_name, num_classes=100, num_channels=3)\n","    else:\n","        raise Exception('dataset is not supported')\n","\n","\n","def network(network_name, num_classes, num_channels):\n","    # Wide ResNet\n","    if network_name == 'wrn':\n","        return wide_resnet_16_4(num_classes)  \n","    # VGG\n","    elif network_name == 'vgg':\n","        return vgg11_bn(num_classes)\n","    else:\n","        raise Exception('network is not supported')\n","\n","\n","\"\"\"\n","===============================\n","Wide ResNets \n","===============================\n","Reimplementation of https://github.com/meliketoy/wide-resnet.pytorch\n","\"\"\"\n","\n","class WideResNet(nn.Module):\n","\n","    def __init__(self, depth, widen_factor, dropout_rate, num_classes):\n","        super(WideResNet, self).__init__()\n","\n","        self.in_planes = 16\n","        \n","        assert (int((depth - 4) % 6) == 0), 'Wide-resnet depth should be 6n+4'\n","        n = int((depth - 4) / 6)\n","        k = widen_factor\n","\n","        n_stages = [16, 16 * k, 32 * k, 64 * k]\n","\n","        self.conv1 = conv3x3(3, n_stages[0])\n","        self.layer1 = self._wide_layer(WideBasicBlock, n_stages[1], n, dropout_rate, stride=1)\n","        self.layer2 = self._wide_layer(WideBasicBlock, n_stages[2], n, dropout_rate, stride=2)\n","        self.layer3 = self._wide_layer(WideBasicBlock, n_stages[3], n, dropout_rate, stride=2)\n","        self.bn1 = nn.BatchNorm2d(n_stages[3], momentum=0.9)\n","        self.fc = nn.Linear(n_stages[3], num_classes)\n","        \n","        self.n_features = n_stages[3]\n","\n","    def _wide_layer(self, block, planes, num_blocks, dropout_rate, stride):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, dropout_rate, stride))\n","            self.in_planes = planes\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, hparam=None):\n","        out = self.conv1(x)\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = F.relu(self.bn1(out))\n","        out = F.avg_pool2d(out, 8)\n","        x = out.view(out.size(0), -1)\n","        if hparam is not None:\n","            assert x.size() == hparam.size(), 'size mismatch'\n","            r = x.data*hparam \n","            x += r\n","        x = self.fc(x)\n","        return x\n","\n","\n","class WideBasicBlock(nn.Module):\n","\n","    def __init__(self, in_planes, planes, dropout_rate, stride=1):\n","        super(WideBasicBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, bias=False)\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n","            )\n","\n","    def forward(self, x):\n","        out = self.dropout(self.conv1(F.relu(self.bn1(x))))\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += self.shortcut(x)\n","        return out\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","def wide_resnet_16_4(num_classes, dropout=0.0):\n","    return WideResNet(16, 4, dropout, num_classes)\n","\n","def wide_resnet_28_10(num_classes, dropout=0.0):\n","    return WideResNet(28, 10, dropout, num_classes)\n","\n","def wide_resnet_28_20(num_classes, dropout=0.0):\n","    return WideResNet(28, 20, dropout, num_classes)\n","\n","\n","\n","'''\n","===============================\n","VGG \n","===============================\n","'''\n","\n","class VGG(nn.Module):\n","    '''\n","    VGG model\n","    '''\n","    def __init__(self, features, num_classes):\n","        super(VGG, self).__init__()\n","        \n","        self.features = features\n","        self.n_features = 512\n","        \n","        self.fc = nn.Sequential(\n","            nn.Linear(512, 512),\n","            nn.ReLU(True),\n","            nn.Linear(512, 512),\n","            nn.ReLU(True),\n","            nn.Linear(512, num_classes), \n","        )\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, hparam=None):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        if hparam is not None:\n","            r = x.data*hparam\n","            x += r \n","        x = self.fc(x)\n","        return x\n","\n","\n","def make_layers(cfg, batch_norm=False):\n","    layers = []\n","    in_channels = 3\n","    for v in cfg:\n","        if v == 'M':\n","            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","        else:\n","            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","            if batch_norm:\n","                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n","            else:\n","                layers += [conv2d, nn.ReLU(inplace=True)]\n","            in_channels = v\n","    return nn.Sequential(*layers)\n","\n","\n","__all__ = [\n","    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n","    'vgg19_bn', 'vgg19',\n","]\n","\n","\n","cfg = {\n","    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',\n","          512, 512, 512, 512, 'M'],\n","}\n","\n","\n","def vgg11(num_classes):\n","    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n","    return VGG(make_layers(cfg['A']), num_classes)\n","\n","\n","def vgg11_bn(num_classes):\n","    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\"\"\"\n","    return VGG(make_layers(cfg['A'], batch_norm=True), num_classes)\n","\n","\n","def vgg13():\n","    \"\"\"VGG 13-layer model (configuration \"B\")\"\"\"\n","    return VGG(make_layers(cfg['B']))\n","\n","\n","def vgg13_bn():\n","    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\"\"\"\n","    return VGG(make_layers(cfg['B'], batch_norm=True))\n","\n","\n","def vgg16():\n","    \"\"\"VGG 16-layer model (configuration \"D\")\"\"\"\n","    return VGG(make_layers(cfg['D']))\n","\n","\n","def vgg16_bn():\n","    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\"\"\"\n","    return VGG(make_layers(cfg['D'], batch_norm=True))\n","\n","\n","def vgg19():\n","    \"\"\"VGG 19-layer model (configuration \"E\")\"\"\"\n","    return VGG(make_layers(cfg['E']))\n","\n","\n","def vgg19_bn():\n","    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n","    return VGG(make_layers(cfg['E'], batch_norm=True))\n","\n","\n","\n","\n","''' LeNet: for MNIST and SVHN.\n","'''\n","class LeNet(nn.Module):\n","\n","    def __init__(self, num_classes, num_channels=1):\n","        super(LeNet, self).__init__()\n","        if num_channels == 1:\n","            self.conv1 = nn.Conv2d(1, 20, 5)\n","            self.conv2 = nn.Conv2d(20, 50, 5)\n","            self.fc1 = nn.Linear(4 * 4 * 50, 120)\n","            self.fc2 = nn.Linear(120, 84)\n","            self.fc3 = nn.Linear(84, num_classes)\n","        else:\n","            self.conv1 = nn.Conv2d(3, 6, 5)\n","            self.conv2 = nn.Conv2d(6, 16, 5)\n","            self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","            self.fc2 = nn.Linear(120, 84)\n","            self.fc3 = nn.Linear(84, num_classes)\n","\n","    def forward(self, x, hparam=None):\n","        out = F.relu(self.conv1(x))\n","        out = F.max_pool2d(out, 2)\n","        out = F.relu(self.conv2(out))\n","        out = F.max_pool2d(out, 2)\n","        out = out.view(out.size(0), -1)\n","\n","        if hparam is not None:\n","            r = out.data*hparam\n","            out += r \n","\n","        out = F.relu(self.fc1(out))\n","        out = F.relu(self.fc2(out))\n","        out = self.fc3(out)\n","        return out\n","\n","def lenet_5(num_classes, num_channels):\n","    return LeNet(num_classes, num_channels)\n","\n","\n","''' Multilayer perceptron: for MNIST and SVHN.\n","'''\n","class MLP(nn.Module):\n","\n","    def __init__(self, num_classes, n_1, n_2):\n","        super(MLP, self).__init__()\n","        self.fc1 = nn.Linear(784, n_1)\n","        self.fc2 = nn.Linear(n_1, n_2)\n","        self.fc3 = nn.Linear(n_2, num_classes)\n","        self.n_features = n_2\n","        \n","    def forward(self, x, hparam=None):\n","        x = x.view(-1, 28 * 28)\n","        out = F.relu(self.fc1(x))\n","        out = F.relu(self.fc2(out))\n","        if hparam is not None:\n","          r = out.data*hparam\n","          out += r\n","        return self.fc3(dout)\n","\n","\n","def mlp(num_classes=10, n_1=512, n_2=256):\n","    return MLP(num_classes, n_1, n_2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":905,"status":"ok","timestamp":1651667579760,"user":{"displayName":"Calum MacLellan","userId":"18398237058780708126"},"user_tz":-60},"id":"JeIYnGDjf03O","outputId":"ba1f8026-6541-4c6d-f046-9147f65841fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(99.5000)\n","tensor(999.9963, dtype=torch.float64)\n","tensor(999.9955, dtype=torch.float64)\n","tensor(999.9945, dtype=torch.float64)\n","tensor(999.9932, dtype=torch.float64)\n","tensor(999.9917, dtype=torch.float64)\n","tensor(999.9898, dtype=torch.float64)\n","tensor(999.9876, dtype=torch.float64)\n","tensor(999.9848, dtype=torch.float64)\n","tensor(999.9814, dtype=torch.float64)\n","tensor(999.9772, dtype=torch.float64)\n","tensor(999.9721, dtype=torch.float64)\n","tensor(999.9659, dtype=torch.float64)\n","tensor(999.9583, dtype=torch.float64)\n","tensor(999.9489, dtype=torch.float64)\n","tensor(999.9375, dtype=torch.float64)\n","tensor(999.9235, dtype=torch.float64)\n","tensor(999.9063, dtype=torch.float64)\n","tensor(999.8854, dtype=torch.float64)\n","tensor(999.8597, dtype=torch.float64)\n","tensor(999.8283, dtype=torch.float64)\n","tensor(999.7899, dtype=torch.float64)\n","tensor(999.7429, dtype=torch.float64)\n","tensor(999.6853, dtype=torch.float64)\n","tensor(999.6149, dtype=torch.float64)\n","tensor(999.5287, dtype=torch.float64)\n","tensor(999.4233, dtype=torch.float64)\n","tensor(999.2942, dtype=torch.float64)\n","tensor(999.1364, dtype=torch.float64)\n","tensor(998.9432, dtype=torch.float64)\n","tensor(998.7069, dtype=torch.float64)\n","tensor(998.4179, dtype=torch.float64)\n","tensor(998.0644, dtype=torch.float64)\n","tensor(997.6322, dtype=torch.float64)\n","tensor(997.1036, dtype=torch.float64)\n","tensor(996.4575, dtype=torch.float64)\n","tensor(995.6679, dtype=torch.float64)\n","tensor(994.7032, dtype=torch.float64)\n","tensor(993.5252, dtype=torch.float64)\n","tensor(992.0872, dtype=torch.float64)\n","tensor(990.3330, dtype=torch.float64)\n","tensor(988.1946, dtype=torch.float64)\n","tensor(985.5901, dtype=torch.float64)\n","tensor(982.4212, dtype=torch.float64)\n","tensor(978.5708, dtype=torch.float64)\n","tensor(973.8996, dtype=torch.float64)\n","tensor(968.2435, dtype=torch.float64)\n","tensor(961.4107, dtype=torch.float64)\n","tensor(953.1794, dtype=torch.float64)\n","tensor(943.2966, dtype=torch.float64)\n","tensor(931.4791, dtype=torch.float64)\n","tensor(917.4162, dtype=torch.float64)\n","tensor(900.7772, dtype=torch.float64)\n","tensor(881.2237, dtype=torch.float64)\n","tensor(858.4278, dtype=torch.float64)\n","tensor(832.0980, dtype=torch.float64)\n","tensor(802.0115, dtype=torch.float64)\n","tensor(768.0511, dtype=torch.float64)\n","tensor(730.2446, dtype=torch.float64)\n","tensor(688.7990, dtype=torch.float64)\n","tensor(644.1233, dtype=torch.float64)\n","tensor(596.8321, dtype=torch.float64)\n","tensor(547.7246, dtype=torch.float64)\n","tensor(497.7377, dtype=torch.float64)\n","tensor(447.8777, dtype=torch.float64)\n","tensor(399.1408, dtype=torch.float64)\n","tensor(352.4351, dtype=torch.float64)\n","tensor(308.5172, dtype=torch.float64)\n","tensor(267.9511, dtype=torch.float64)\n","tensor(231.0943, dtype=torch.float64)\n","tensor(198.1061, dtype=torch.float64)\n","tensor(168.9746, dtype=torch.float64)\n","tensor(143.5525, dtype=torch.float64)\n","tensor(121.5964, dtype=torch.float64)\n","tensor(102.8030, dtype=torch.float64)\n","tensor(86.8397, dtype=torch.float64)\n","tensor(73.3685, dtype=torch.float64)\n","tensor(62.0629, dtype=torch.float64)\n","tensor(52.6184, dtype=torch.float64)\n","tensor(44.7592, dtype=torch.float64)\n","tensor(38.2401, dtype=torch.float64)\n","tensor(32.8471, dtype=torch.float64)\n","tensor(28.3955, dtype=torch.float64)\n","tensor(24.7276, dtype=torch.float64)\n","tensor(21.7101, dtype=torch.float64)\n","tensor(19.2306, dtype=torch.float64)\n","tensor(17.1953, dtype=torch.float64)\n","tensor(15.5260, dtype=torch.float64)\n","tensor(14.1579, dtype=torch.float64)\n","tensor(13.0372, dtype=torch.float64)\n","tensor(12.1196, dtype=torch.float64)\n","tensor(11.3686, dtype=torch.float64)\n","tensor(10.7541, dtype=torch.float64)\n","tensor(10.2515, dtype=torch.float64)\n","tensor(9.8404, dtype=torch.float64)\n","tensor(9.5043, dtype=torch.float64)\n","tensor(9.2294, dtype=torch.float64)\n","tensor(9.0048, dtype=torch.float64)\n","tensor(8.8211, dtype=torch.float64)\n","tensor(8.6710, dtype=torch.float64)\n","tensor(8.5484, dtype=torch.float64)\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","import torch\n","import torch.nn as nn\n","\n","criterion = nn.SmoothL1Loss()\n","label = torch.tensor(float(200))\n","pred = torch.tensor(float(100))\n","loss = criterion(pred, label)\n","print(loss)\n","\n","\n","x = np.linspace(-10, 10,100)\n","x = torch.tensor(x)\n","y_torch = torch.sigmoid(-x+10)\n","fig1 = plt.figure()\n","plt.plot(x, y_torch)\n","\n","def ctbs1(s, B_min=8, B_max=1000):\n","    B = (B_max - B_min)*torch.sigmoid(s) + B_min\n","    return int(np.floor(B.item()))\n","\n","def ctbs2(s, B_min=8, B_max=1000, warmup=False):\n","    B = (B_max - B_min)*torch.sigmoid(-s+2.5) + B_min\n","    print(B)\n","    # if warmup:\n","    # else:\n","    return int(np.floor(B.item()))\n","\n","b1 = np.array([ctbs1(i) for i in x])\n","b2 = np.array([ctbs2(i) for i in x])\n","\n","fig2 = plt.figure()\n","plt.plot(x,b1)\n","plt.plot(x,b2)\n","plt.xlabel('sample')\n","plt.ylabel('batch size')\n","plt.legend([r'1/1+exp(-x)',r'1/1+exp(x-5)'])\n","# s = torch.tensor(float(10))\n","# B = ctbs()\n","# plot()\n","\n","\n","def stable_sigmoid(x):\n","    sig = 1/(1 + np.exp(-x))\n","    # sig = np.where(x < 0, np.exp(x)/(1 + np.exp(x)), 1/(1 + np.exp(-x)))\n","    return sig\n","\n","def logit(y, eps=1e-4):\n","    if y == 0:\n","      y+=eps\n","    elif y==1:\n","      y-=eps      \n","    return np.log(y) - np.log(1-y)\n","\n","# x = np.linspace(-100,100,100)\n","# print(x)\n","# y = stable_sigmoid(x)\n","# print(y)\n","# plot(x,y)\n","\n","# z = np.array([logit(i) for i in y])\n","# print(z)"]},{"cell_type":"markdown","metadata":{"id":"8R3NQg3TDnia"},"source":["# utils.py"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"N5MsD6qDDpay","executionInfo":{"status":"ok","timestamp":1660128442755,"user_tz":-60,"elapsed":1041,"user":{"displayName":"Calum MacLellan","userId":"18398237058780708126"}}},"outputs":[],"source":["\n","\n","import math\n","import os\n","import cv2\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.autograd as ag\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","def warmup(scheduler, optimiser, dataloaders, save=False):\n","    print('\\nwarming up...')\n","    s_target = dataloaders['train'].batch_size\n","    huber = nn.SmoothL1Loss().cuda()\n","    temp_hyperlr = 1e-4\n","    optimiser.param_groups[0]['lr'] = optimiser.param_groups[1]['lr'] = temp_hyperlr\n","    l_init = ctl(s_target).cuda() \n","    print(l_init) \n","    scheduler.train()\n","    for j in range(10000): # arbitrarily high iteration length: usually only takes around 1 to 2 epochs (2000 iters at most)\n","        # inputs, _ = next(iter(dataloaders['val']))\n","        inputs = torch.randn(s_target, 3, 32, 32)\n","        optimiser.zero_grad()\n","        logits, l_alpha, S_alpha, _ = scheduler(inputs.cuda(), warmup=True)\n","        distance = huber(l_alpha, l_init)\n","        # print(distance)\n","        distance.backward()\n","        optimiser.step()\n","        print(S_alpha)\n","        if np.abs(S_alpha - s_target) < 2:\n","            if save:\n","                torch.save(scheduler.state_dict(), \n","                  os.path.join(root_path, './warmedUp_scheduler_s={}.pth.tar'.format(batch_size))\n","                )\n","            optimiser.param_groups[0]['lr'] = phi_lr\n","            optimiser.param_groups[1]['lr'] = alpha_lr\n","            optimiser.param_groups[1]['params'] = scheduler.alpha_params(reset=True)\n","            break\n","    assert optimiser.param_groups[0]['lr'] == phi_lr, 'Phi has the wrong hyper lr.'\n","    assert optimiser.param_groups[1]['lr'] == alpha_lr, 'Alpha has the wrong hyper lr.'\n","    if any(abs(optimiser.param_groups[1]['params'][0]) > 1e-4) == True:\n","        raise AssertionError ('alphas havent been reset. Make sure they are before starting training.')\n","\n","# def warmup(scheduler, optimiser, dataloaders, save=False):\n","#     print('\\nwarming up...')\n","#     B_init = dataloaders['train'].batch_size\n","#     B_init = torch.tensor(float(B_init)).cuda()\n","#     huber = nn.SmoothL1Loss().cuda()\n","#     temp_hyperlr = 1e-4\n","#     optimiser.param_groups[0]['lr'] = optimiser.param_groups[1]['lr'] = temp_hyperlr\n","#     scheduler.train()\n","#     for j in range(10000): # arbitrarily high iteration length: usually only takes around 1 to 2 epochs (2000 iters at most)\n","#         inputs, _ = next(iter(dataloaders['val']))\n","#         optimiser.zero_grad()\n","#         logits, l_alpha, B_pred = scheduler(inputs.cuda(), warmup=True)\n","#         # print(B_pred)\n","#         distance = huber(B_pred, B_init)\n","#         distance.backward()\n","#         optimiser.step()\n","#         # print(S_alpha)\n","#         if torch.abs(B_pred - B_init) < 1:\n","#             if save:\n","#                 torch.save(scheduler.state_dict(), \n","#                   os.path.join(root_path, './warmedUp_scheduler_s={}.pth.tar'.format(dataloaders['train'].batch_size))\n","#                 )\n","#             optimiser.param_groups[0]['lr'] = phi_lr\n","#             optimiser.param_groups[1]['lr'] = alpha_lr\n","#             optimiser.param_groups[1]['params'] = scheduler.alpha_params(reset=True)\n","#             break\n","#     assert optimiser.param_groups[0]['lr'] == phi_lr, 'Phi has the wrong hyper lr.'\n","#     assert optimiser.param_groups[1]['lr'] == alpha_lr, 'Alpha has the wrong hyper lr.'\n","#     if any(abs(optimiser.param_groups[1]['params'][0]) > 1e-4) == True:\n","#         raise AssertionError ('alphas havent been reset. Make sure they are before starting training.')\n","\n","\n","def logit(x):\n","    return torch.log(x) - torch.log(1-x)\n","\n","def ctl(c,s_min=8,s_max=1000,eps=1e-3):\n","    \n","    bs = torch.tensor(float(c))\n","    if c == s_min:\n","        bs += eps\n","    elif c == s_max:\n","        bs -= eps\n"," \n","    return logit((bs - s_min) / (s_max - s_min))\n","\n","def ctbs(l, s_min=8, s_max=1000):\n","    \n","    x = (s_max - s_min)*torch.sigmoid(l) + s_min\n","    return int(np.floor(x.item()))\n","\n","def inv_softplus(x):\n","    \"\"\" Inverse softplus function: Maps x lying in (0, infty) to R\"\"\"\n","    return torch.log(torch.exp(x) - 1)\n","\n","def bs_softplus(x, s_min=8, s_max=1000):\n","    x = (s_max - s_min)*nn.functional.softplus(x) + s_min\n","    return int(np.floor(x.item()))\n","\n","\n","# def ctbs(s, B_min=8, B_max=512, warmup=False):\n","#     B = (B_max - B_min)*torch.sigmoid(-s+2.5) + B_min\n","#     if warmup:\n","#       return B\n","#     else:\n","#       return int(np.floor(B.item()))\n","\n","def init_weights(m):\n","    if type(m) == nn.Conv2d:\n","        init.xavier_uniform_(m.weight, gain=np.sqrt(2))\n","    elif type(m) == nn.BatchNorm2d:\n","        init.constant_(m.weight, 1)\n","        init.constant_(m.bias, 0)\n","\n","\n","def count_correct(output, target):\n","    probs = F.softmax(output,dim=1)\n","    _, pred = torch.max(probs.data, 1)\n","    correct = (pred == target).sum().item()\n","    return correct\n","\n","def read_results(file):\n","    with open(file) as f:\n","        array=[]\n","        for line in f.readlines():\n","            array.append(np.array(line).astype(np.float))\n","        f.close()\n","    return array\n","\n","\n","def write_csv(stuff, file, batch_size=True):\n","    x_np = np.array(stuff)\n","    x_df = pd.DataFrame(x_np)\n","    x_df.to_csv(file)\n","\n","  \n","def write_dist(stuff, file):\n","    if len(stuff)==1:\n","        info = str(stuff[0])\n","    elif len(stuff)==4:\n","        info = str(stuff[0])+' '+str(stuff[1])+' '+str(stuff[2])+' '+str(stuff[3])\n","    elif len(stuff)==10:\n","        info = str(stuff[0])+' '+str(stuff[1])+' '+str(stuff[2])+' '+str(stuff[3])+' '+str(stuff[4])+' '+str(stuff[5])+' '+str(stuff[6])+' '+str(stuff[7])+' '+str(stuff[8])+' '+str(stuff[9])\n","    file.write(info+'\\n')\n","    file.flush()\n","    \n","    \n","def write_results(stuff, file):\n","    file.write(stuff+'\\n')\n","    file.flush()\n","\n","\n","\n","def clear_grad(model):\n","    for p in model.parameters():\n","        if p.grad is not None:\n","            p.grad.detach()\n","            p.grad.zero_()\n","\n","\n","def count_params(net):\n","    return sum(p.numel() for p in net.parameters())/1e6\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1651678944112,"user":{"displayName":"Calum MacLellan","userId":"18398237058780708126"},"user_tz":-60},"id":"Qcru6DftuBfn","outputId":"876fb6a8-57c0-4135-a8cf-4d7d59c15178"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(0.4741)\n","tensor(0.3775)\n","246\n","198\n"]}],"source":["x = torch.tensor(-0.5)\n","b = torch.nn.functional.softplus(x)\n","b2 = torch.sigmoid(x)\n","print(b)\n","print(b2)\n","\n","print(bs_softplus(x))\n","print(ctbs(x))"]},{"cell_type":"markdown","metadata":{"id":"Nx_i_Jaj9SAc"},"source":["# Arbiter.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DB0Uc9N09SrP"},"outputs":[],"source":["\n","'''\n","@startdate: 13/10/21.\n","@author: calmac\n","'''\n","\n","\n","import torch\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import numpy as np\n","import os \n","\n","class bsg(nn.Module):\n","   \n","    def __init__(self, n_samples, n_features, results_files):\n","        super(bsg, self).__init__()\n","        \n","        self.n_samples = n_samples\n","        self.results_files = results_files\n","        \n","        # self.loss_embedding = nn.Embedding()\n","\n","        self.layer1 = nn.Sequential(\n","              nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=0),\n","              nn.BatchNorm2d(64),\n","              nn.ReLU(inplace=True),\n","              )\n","      \n","        self.layer2 = nn.Sequential(\n","                nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n","                nn.BatchNorm2d(64),\n","                nn.ReLU(inplace=True),\n","                )\n","        \n","        self.layer3 = nn.Sequential(\n","                nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n","                nn.BatchNorm2d(64),\n","                nn.ReLU(inplace=True),\n","                )\n","        \n","        self.layer4 = nn.Sequential(\n","                nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n","                nn.BatchNorm2d(64),\n","                nn.ReLU(inplace=True),\n","                )\n","        \n","        self.pooling = nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Linear(64, self.n_samples)\n","        self.alphas = Variable(1e-6*torch.randn(self.n_samples).cuda(), requires_grad=True)\n","        self.linear = nn.Linear(1, n_features, bias=False)\n","        # if mapping_grads_off:\n","        #     for p in self.linear.parameters():\n","        #         p.requires_grad_(False) \n","\n","        self.reset = False\n","        self.im_we = torch.ones(n_samples).cuda()\n","        self.im_we[-1:] *= 100\n","\n","    def alpha_params(self, reset=False):\n","       \n","        if reset:\n","            self.alphas = Variable(1e-6*torch.randn(self.n_samples).cuda(), requires_grad=True)\n","        return [self.alphas]\n","\n","    def _transform(self, x):\n","        x = x.transpose(-1, -2)   \n","        x = torch.mean(x, dim=1)  \n","        x = torch.sort(x)[0]     \n","        return x\n","\n","\n","    def forward(self, data, warmup=False):\n","        \n","        x = self.layer1(data)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        x = self.pooling(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        \n","        logits = self._transform(x)\n","        weights = F.softmax(self.alphas, dim=-1)\n","        l_alpha = sum(l*w for l, w in zip(logits, weights))\n","        S_alpha = ctbs(l_alpha)\n","        batch_l = l_alpha.repeat(S_alpha, 1)\n","        hparam = self.linear(batch_l)\n","        \n","        if not warmup:\n","            write_csv([ctbs(l) for l in logits], self.results_files['samples'])\n","            write_csv([w.item() for w in weights], self.results_files['alpha'])\n","            write_results('{}'.format(l_alpha.item()), self.results_files['la'])\n","            write_results('{}'.format(S_alpha), self.results_files['sa'])\n","        \n","        return logits, l_alpha, S_alpha, hparam\n","    "]},{"cell_type":"markdown","metadata":{"id":"2oLKxsRR-4hl"},"source":["# main.py"]},{"cell_type":"markdown","metadata":{"id":"of4vxSzzHVJX"},"source":["code for changing batch size at 60 epoch (Smith heuristic)\n"," i.e. not changed each epoch as before "]},{"cell_type":"code","execution_count":37,"metadata":{"id":"8c6FfUBX-7Sk","executionInfo":{"status":"ok","timestamp":1660132902183,"user_tz":-60,"elapsed":1049,"user":{"displayName":"Calum MacLellan","userId":"18398237058780708126"}}},"outputs":[],"source":["'''\n","\n","@latestUpdate: 17/11/21\n","@dateStarted: 13/10/21\n","@author: calmac\n","'''\n","import time\n","import os\n","import argparse\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","\n","\n","def main(results_files):\n","    \n","    dataloaders = get_dataloaders(dataset, batch_size)\n","    net = get_network(arch, dataset).cuda()\n","\n","    if lr_scheduler_type == 'hd':\n","        print('\\nhd lr decay on ({}).'.format(optim_type))\n","        if optim_type == 'adam':\n","            net_optimiser = AdamHD(net.parameters(), lr=lr, weight_decay=wd, hypergrad_lr=hd_hyperlr, results_files=results_files)\n","        else:\n","            net_optimiser = SGDHD(net.parameters(), lr=lr, momentum=momentum, weight_decay=wd, hypergrad_lr=hd_hyperlr, results_files=results_files)\n","    else:\n","        if optim_type == 'adam':\n","          net_optimiser = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n","        else:\n","          net_optimiser = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=wd)\n","\n","        if lr_scheduler_type == 'exp':\n","            lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(net_optimiser, gamma=lr_decay)\n","        elif lr_scheduler_type == 'step':\n","            print('\\nstep lr decay on.')\n","            lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(net_optimiser, milestones=milestones, gamma=lr_decay)\n","        elif lr_scheduler_type == 'cosine':\n","            print('\\ncosine lr decay on.')\n","            lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(net_optimiser, T_0=t_0, T_mult=t_mult, eta_min=lr * 1e-2)\n","        elif lr_scheduler_type == 'off':\n","            print('\\nlr decay off.')\n","        lr_scheduler = None\n","\n","    bs_scheduler = bsg(n_samples=n_samples,n_features=net.n_features, results_files=results_files).cuda()\n","    print('# parameters in Arbiter: {}'.format(sum(p.numel() for p in bs_scheduler.parameters())))\n","\n","    sched_optimiser = torch.optim.Adam([\n","                                    {'params':bs_scheduler.parameters(),   'lr':phi_lr},\n","                                    {'params':bs_scheduler.alpha_params(), 'lr':alpha_lr},\n","                                    ], weight_decay=wd)\n","    optimisers = {'net':net_optimiser, 'sched':sched_optimiser}\n","\n","    ''' misc\n","    '''\n","    criterion = nn.CrossEntropyLoss().cuda()\n","    num_train = int(train_percent*len(dataloaders['train'].dataset))\n","    num_val = int(np.round(1-train_percent, 1)*len(dataloaders['val'].dataset)) \n","    num_test = int(len(dataloaders['test'].dataset))\n","\n","    ''' warmup: bring samples around current value.\n","    '''\n","    if use_warm:\n","        warmsched_weights = './warmedUp_scheduler_s={}.pth.tar'.format(batch_size)\n","        if not os.path.exists(warmsched_weights): \n","            raise AssertionError('You want to use warmed up but no pretrained weights found.')\n","        else:\n","            bs_scheduler.load_state_dict(torch.load(warmsched_weights))\n","    else:\n","        warmup(bs_scheduler, sched_optimiser, dataloaders)\n","        print('done')\n","\n","    best_acc = 0.0\n","    new_dataloaders = None\n","    scale_time = False\n","    turn_off = False\n","\n","    write_results('{:4.6f}'.format(lr), results_files['lrit'])\n","    write_results('{:4.6f}'.format(lr), results_files['lrep'])\n","    write_results('{}'.format(batch_size), results_files['bse'])\n","\n","    for epoch in range(num_epochs):\n","\n","        if new_dataloaders is not None:\n","            dataloaders = new_dataloaders\n","        \n","        print()\n","        print('Epoch {} stats:'.format(epoch+1))\n","        \n","        cur_lr = net_optimiser.param_groups[0]['lr']\n","        cur_bs = dataloaders['train'].batch_size\n","        print('start of epoch lr: {}'.format(cur_lr))\n","        print('start of epoch BS: {}'.format(cur_bs))\n","        print('            LR/BS: {}'.format(cur_lr/cur_bs))\n","        write_results('{}'.format(cur_lr), results_files['lrep'])  \n","        write_results('{}'.format(cur_bs), results_files['bse'])  \n","        write_results('{}'.format(cur_lr/cur_bs), results_files['ratio'])  \n","       \n","        \n","        ''' \n","        ======================\n","        Training epoch \n","        ======================\n","        '''\n","        t_start = time.time()\n","        if not turn_off:\n","            logits, train_stats  = train(net, bs_scheduler, dataloaders, criterion, optimisers, turn_off=turn_off)\n","        else:\n","            train_stats  = train(net, bs_scheduler, dataloaders, criterion, optimisers, turn_off=turn_off)\n","\n","        print('epoch took {}s'.format(time.time() - t_start))\n","        write_results('{}'.format(time.time() - t_start), results_files['times'])\n","\n","        if lr_scheduler_type != 'hd' and lr_scheduler is not None:\n","          lr_scheduler.step()\n","\n","        train_acc = 100. * (train_stats['correct'] / num_train)\n","        print(\n","              '\\tTraining accuracy: {}/{} ({}%)'.format(\n","              train_stats['correct'], num_train, train_acc\n","              )\n","        )\n","        print('\\tTraining loss: {}'.format(train_stats['loss']))\n","        \n","        ''' \n","        ======================\n","        Validation epoch \n","        ======================\n","        '''\n","        val_stats = validation(net, dataloaders['val'], criterion)\n","        val_acc = 100. * (val_stats['correct'] / num_val)\n","        print(\n","              '\\tValidation accuracy: {}/{} ({}%)'.format(\n","              val_stats['correct'], num_val, val_acc\n","              )\n","        )\n","        print('\\tValidation loss: {}'.format(val_stats['loss']))\n","        \n","        ''' \n","        ======================\n","        Testing epoch \n","        ======================\n","        '''\n","        test_stats   = test(net, dataloaders['test'], criterion)\n","        test_acc = 100. * (test_stats['correct'] / num_test)\n","        print(\n","              '\\tTest accuracy: {}/{} ({}%)'.format(\n","              test_stats['correct'], num_test, test_acc\n","              )\n","        )\n","        print('\\tTest loss: {}'.format(test_stats['loss']))\n","\n","        # ''' View Arbiters progress\n","        # '''\n","        # if not turn_off:\n","        #     print('\\nArbiters progress:')\n","        #     alphas = sched_optimiser.param_groups[1]['params'][0]\n","        #     weights = F.softmax(alphas, dim=-1)\n","        #     logit_best = logits[torch.max(weights, dim=0)[1]]\n","        #     xplus1 = ctbs(logit_best)\n","        #     samples = [ctbs(l) for l in logits]\n","        #     print('\\tsamples: {}'.format(samples))\n","        #     print('\\talphas: {}'.format([w.item() for w in weights]))\n","        #     print('\\tbest sample: {}'.format(xplus1))\n","        #     write_results('{}'.format(xplus1), results_files['best'])  \n","        \n","        ''' Scheduling time \n","        '''\n","        if not turn_off and (epoch+1) % sched_int == 0:\n","            alphas = sched_optimiser.param_groups[1]['params'][0]\n","            weights = F.softmax(alphas, dim=-1)\n","            logit_best = logits[torch.max(weights, dim=0)[1]]\n","            xplus1 = ctbs(logit_best)\n","            samples = [ctbs(l) for l in logits]\n","            print('samples: {}'.format(samples))\n","            print('alphas: {}'.format([w.item() for w in weights]))\n","            print('best sample: {}'.format(xplus1))\n","            new_dataloaders = get_dataloaders(dataset, batch_size=xplus1)\n","            assert xplus1 == new_dataloaders['train'].batch_size, 'Mismatch between batch sizes.'\n","            # write_results('{}'.format(xplus1), results_files['bse'])\n","            write_csv([ctbs(l) for l in logits], results_files['end-samples'])\n","            write_csv([w.item() for w in weights], results_files['end-alpha'])\n","            torch.save(bs_scheduler.state_dict(), \n","              os.path.join(arb_dir, './arbiter_bestSample={}_epoch={}.pth.tar'.format(xplus1, epoch+1))\n","            )\n","            sched_optimiser.param_groups[1]['params'] = bs_scheduler.alpha_params(reset=True)\n","            \n","        ''' Run schedule \n","        ''' \n","        if bs_scheduling:\n","          if not smith_heuristic and (epoch+1) in bs_milestone:\n","            idx = bs_milestone.index(epoch+1)\n","            old_batch_size = dataloaders['train'].batch_size\n","            new_batch_size = bs[idx]\n","            print('batch size changed from {} to {}.'.format(old_batch_size, new_batch_size))\n","            new_dataloaders = get_dataloaders(dataset, batch_size=new_batch_size)\n","            \n","            ''' bring phi samples close to new batch size\n","            '''\n","            warmup(bs_scheduler, sched_optimiser, new_dataloaders) \n","            print('done')\n","\n","\n","          elif smith_heuristic and (epoch+1) in smith_milestones:\n","            ''' implement hybrid heuristic from Smith 2018\n","            '''\n","            # batch size: only change once at 60 epochs then constant.\n","            if (epoch+1) == smith_milestones[0]:\n","              # old_batch_size = dataloaders['train'].batch_size # most recent batch size set by Arbiter\n","              # new_batch_size = xplus1 \n","              # # new_batch_size = 5*batch_size                    # scale original batch size by Smiths heuristic (x5)\n","              # new_dataloaders = get_dataloaders(dataset, batch_size=new_batch_size)\n","              # # print('batch size changed from {} to {}.'.format(old_batch_size, new_batch_size))\n","              print('reached epoch 60, but continuing with Arbiter....')\n","\n","              ''' new idea (29/4)\n","                  let arbiter solve for next batch size at 60th epoch, then remove arbiter entirely and let\n","                  the rest of Smiths heuristic play out with that batch size.\n","              '''\n","              # turn_off = True\n","              # print('Arbiter turned off.')#Returned to normal training under his predicted batch size of {}.'.format(new_dataloaders['train'].batch_size))\n","              \n","              ''' newer idea (9/5/22)\n","                    LET Arbiter schedule the batch size until Smiths heuristic change to B at 60 epochs.\n","                    increase to B=640 then remove Arbiter. \n","                    idea is to show that we can achieve similar dynamics and performance with even less param updates \n","                    by slightly increasing B with Arbiter's heuristics.\n","                    THEN, read the schedule and apply to a new WRN to see if results generalise well.\n","              '''\n","\n","            elif (epoch+1) in smith_milestones:\n","              # learning rate: only change at 120 and 160 (see milestones in MultiStepLR call)\n","              old_lr = net_optimiser.param_groups[0]['lr']\n","              print('old lr: {}'.format(old_lr))\n","              net_optimiser.param_groups[0]['lr'] *= 0.2\n","              # lr_scheduler.step()\n","              print('new lr: {}'.format(net_optimiser.param_groups[0]['lr']))\n","              \n","  \n","        if save_results:\n","            write_results('{:.6f}'.format(train_stats['loss']), results_files['trl'])\n","            write_results('{:.6f}'.format(train_acc), results_files['tra'])\n","            write_results('{:.6f}'.format(val_stats['loss']), results_files['vl'])\n","            write_results('{:.6f}'.format(val_acc), results_files['va'])\n","            write_results('{:.6f}'.format(test_stats['loss']), results_files['tel'])\n","            write_results('{:.6f}'.format(test_acc), results_files['tea'])\n","        \n","        if val_acc > best_acc:\n","            best_acc = val_acc\n","            print('best val acc so far: {}'.format(best_acc))\n","        if save_weights:\n","            torch.save(\n","              net.state_dict(), \n","              os.path.join(model_dir, '{}_{}_valLoss={:4.4f}_valAcc={:4.4f}_epoch={}.pth.tar'.format(arch, dataset, val_stats['loss'], val_acc, epoch+1))\n","            )   \n","               \n","def train(net, bs_scheduler, dataloaders, criterion, optimisers, turn_off):\n","    \n","    net.train()   \n","    bs_scheduler.train()\n","    net_optimiser = optimisers['net']\n","    sched_optimiser = optimisers['sched']\n","    correct = 0\n","    loss_list = []\n","    \n","    for i, (inputs, targets) in enumerate(dataloaders['train']):\n","\n","        write_results('{}'.format(dataloaders['train'].batch_size), results_files['bsi'])\n","        \n","        net_optimiser.zero_grad()\n","        sched_optimiser.zero_grad()\n","        custom_dataloader = dataloaders['custom'] \n","            \n","        inputs, targets = inputs.cuda(), targets.cuda()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss_list.append(loss.item())\n","        correct += count_correct(outputs, targets)\n","        loss.backward()\n","        net_optimiser.step() \n","\n","        if not turn_off:\n","            val_inputs, _ = next(iter(dataloaders['val']))\n","            logits, l_alpha, S_alpha, hparam = bs_scheduler(val_inputs.cuda())\n","            ### NEW IDEA (12/7/22): condition batch size preds on current val_loss\n","            outputs = net(inputs)\n","            # info_loss = criterion(outputs, targets)\n","            # noise = torch.randn(dataloaders['val'].batch_size, 3, 32, 32)\n","            # logits, l_alpha, S_alpha, hparam = bs_scheduler(noise.cuda(), info_loss)\n","            new_data, new_targets = custom_dataloader(sample_size=S_alpha)\n","            new_data, new_targets = new_data.cuda(), new_targets.cuda()\n","            \n","            new_outputs = net(new_data, hparam=hparam)  \n","            # ce = criterion(new_outputs, new_targets)\n","            # F = ce + (ce - info_loss)\n","            F = criterion(new_outputs, new_targets)\n","            F.backward()\n","            sched_optimiser.step()\n","            write_results('{}'.format(F.item()), results_files['F'])             \n","        \n","    if not turn_off:\n","        return logits, {'correct': correct, 'loss': np.mean(loss_list)}\n","    else:\n","        return {'correct': correct, 'loss': np.mean(loss_list)}\n","\n","\n","\n","def validation(net, dataloader, criterion):\n","    net.eval()\n","    correct = 0\n","    loss_list = []\n","    with torch.no_grad():\n","        for i, (data, target) in enumerate(dataloader):\n","            data, target = data.cuda(), target.cuda()\n","            output = net(data) \n","            loss = criterion(output, target)\n","            loss_list.append(loss.item())\n","            correct += count_correct(output, target)\n","    return {'correct': correct, 'loss': np.mean(loss_list)}\n","\n","\n","def test(net, dataloader, criterion):\n","    net.eval()\n","    correct = 0\n","    loss_list = []\n","    with torch.no_grad():\n","        for i, (data, target) in enumerate(dataloader):\n","            data, target = data.cuda(), target.cuda()\n","            output = net(data) \n","            loss = criterion(output, target)\n","            loss_list.append(loss.item())\n","            correct += count_correct(output, target)\n","    return {'correct': correct, 'loss': np.mean(loss_list)}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZR1uu5YgFq3G"},"source":["draft: 30/4/22"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yv77-9xsFop4"},"outputs":[],"source":["# '''\n","\n","# @latestUpdate: 17/11/21\n","# @dateStarted: 13/10/21\n","# @author: calmac\n","# '''\n","# import time\n","# import os\n","# import argparse\n","# import numpy as np\n","# import torch\n","# import torch.nn as nn\n","# from torch.autograd import Variable\n","# import torch.nn.functional as F\n","\n","\n","# def main(results_files):\n","#     print('wrong main')\n","#     dataloaders = get_dataloaders(dataset, batch_size)\n","#     net = get_network(arch, dataset).cuda()\n","\n","#     if lr_scheduler_type == 'hd':\n","#         print('\\nhd lr decay on ({}).'.format(optim_type))\n","#         if optim_type == 'adam':\n","#             net_optimiser = AdamHD(net.parameters(), lr=lr, weight_decay=wd, hypergrad_lr=hd_hyperlr, results_files=results_files)\n","#         else:\n","#             net_optimiser = SGDHD(net.parameters(), lr=lr, momentum=momentum, weight_decay=wd, hypergrad_lr=hd_hyperlr, results_files=results_files)\n","#     else:\n","#         if optim_type == 'adam':\n","#           net_optimiser = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n","#         else:\n","#           net_optimiser = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=wd)\n","\n","#         if lr_scheduler_type == 'exp':\n","#             lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(net_optimiser, gamma=lr_decay)\n","#         elif lr_scheduler_type == 'step':\n","#             print('\\nstep lr decay on.')\n","#             lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(net_optimiser, milestones=milestones, gamma=lr_decay)\n","#         elif lr_scheduler_type == 'cosine':\n","#             print('\\ncosine lr decay on.')\n","#             lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(net_optimiser, T_0=t_0, T_mult=t_mult, eta_min=lr * 1e-2)\n","#         elif lr_scheduler_type == 'off':\n","#             print('\\nlr decay off.')\n","#         lr_scheduler = None\n","\n","#     bs_scheduler = bsg(n_samples=n_samples,n_features=net.n_features, results_files=results_files).cuda()\n","#     print('# parameters in Arbiter: {}'.format(sum(p.numel() for p in bs_scheduler.parameters())))\n","\n","#     sched_optimiser = torch.optim.Adam([\n","#                                     {'params':bs_scheduler.parameters(),   'lr':phi_lr},\n","#                                     {'params':bs_scheduler.alpha_params(), 'lr':alpha_lr},\n","#                                     ], weight_decay=wd)\n","#     optimisers = {'net':net_optimiser, 'sched':sched_optimiser}\n","\n","#     ''' misc\n","#     '''\n","#     criterion = nn.CrossEntropyLoss().cuda()\n","#     num_train = int(train_percent*len(dataloaders['train'].dataset))\n","#     num_val = int(np.round(1-train_percent, 1)*len(dataloaders['val'].dataset)) \n","#     num_test = int(len(dataloaders['test'].dataset))\n","\n","#     ''' warmup: bring samples around current value.\n","#     '''\n","#     if use_warm:\n","#         warmsched_weights = './warmedUp_scheduler_s={}.pth.tar'.format(batch_size)\n","#         if not os.path.exists(warmsched_weights): \n","#             raise AssertionError('You want to use warmed up but no pretrained weights found.')\n","#         else:\n","#             bs_scheduler.load_state_dict(torch.load(warmsched_weights))\n","#     else:\n","#         warmup(bs_scheduler, sched_optimiser, dataloaders)\n","#         print('done')\n","\n","#     best_acc = 0.0\n","#     new_dataloaders = None\n","#     scale_time = False\n","#     turn_off = False\n","\n","#     write_results('{:4.6f}'.format(lr), results_files['lrit'])\n","#     write_results('{:4.6f}'.format(lr), results_files['lrep'])\n","#     write_results('{}'.format(batch_size), results_files['bse'])\n","\n","#     for epoch in range(num_epochs):\n","\n","#         if new_dataloaders is not None:\n","#             dataloaders = new_dataloaders\n","        \n","#         print()\n","#         print('Epoch {} stats:'.format(epoch+1))\n","        \n","#         ''' \n","#         ======================\n","#         Training epoch \n","#         ======================\n","#         '''\n","#         t_start = time.time()\n","#         if not turn_off:\n","#             logits, train_stats  = train(net, bs_scheduler, dataloaders, criterion, optimisers, turn_off=turn_off)\n","#         else:\n","#             train_stats  = train(net, bs_scheduler, dataloaders, criterion, optimisers, turn_off=turn_off)\n","\n","#         print('epoch took {}s'.format(time.time() - t_start))\n","#         write_results('{}'.format(time.time() - t_start), results_files['times'])\n","\n","#         if lr_scheduler_type != 'hd' and lr_scheduler is not None:\n","#           lr_scheduler.step()\n","\n","#         train_acc = 100. * (train_stats['correct'] / num_train)\n","#         print(\n","#               '\\tTraining accuracy: {}/{} ({}%)'.format(\n","#               train_stats['correct'], num_train, train_acc\n","#               )\n","#         )\n","#         print('\\tTraining loss: {}'.format(train_stats['loss']))\n","        \n","#         ''' \n","#         ======================\n","#         Validation epoch \n","#         ======================\n","#         '''\n","#         val_stats = validation(net, dataloaders['val'], criterion)\n","#         val_acc = 100. * (val_stats['correct'] / num_val)\n","#         print(\n","#               '\\tValidation accuracy: {}/{} ({}%)'.format(\n","#               val_stats['correct'], num_val, val_acc\n","#               )\n","#         )\n","#         print('\\tValidation loss: {}'.format(val_stats['loss']))\n","        \n","#         ''' \n","#         ======================\n","#         Testing epoch \n","#         ======================\n","#         '''\n","#         test_stats   = test(net, dataloaders['test'], criterion)\n","#         test_acc = 100. * (test_stats['correct'] / num_test)\n","#         print(\n","#               '\\tTest accuracy: {}/{} ({}%)'.format(\n","#               test_stats['correct'], num_test, test_acc\n","#               )\n","#         )\n","#         print('\\tTest loss: {}\\n'.format(test_stats['loss']))\n","\n","#         # ''' View Arbiters progress\n","#         # '''\n","#         # print('\\narbiters progress:')\n","#         # alphas = sched_optimiser.param_groups[1]['params'][0]\n","#         # weights = F.softmax(alphas, dim=-1)\n","#         # logit_best = logits[torch.max(weights, dim=0)[1]]\n","#         # xplus1 = ctbs(logit_best)\n","#         # samples = [ctbs(l) for l in logits]\n","#         # print('\\tsamples: {}'.format(samples))\n","#         # print('\\talphas: {}'.format([w.item() for w in weights]))\n","#         # print('\\tbest sample: {}'.format(xplus1))\n","#         # write_results('{}'.format(xplus1), results_files['best'])  \n","\n","#         ''' Scheduling time \n","#         '''\n","#         if (epoch+1) % sched_int == 0:\n","#             alphas = sched_optimiser.param_groups[1]['params'][0]\n","#             weights = F.softmax(alphas, dim=-1)\n","#             logit_best = logits[torch.max(weights, dim=0)[1]]\n","#             xplus1 = ctbs(logit_best)\n","#             samples = [ctbs(l) for l in logits]\n","#             print('samples: {}'.format(samples))\n","#             print('alphas: {}'.format([w.item() for w in weights]))\n","#             print('s for the next epoch: {}'.format(xplus1))\n","#             new_dataloaders = get_dataloaders(dataset, batch_size=xplus1)\n","#             assert xplus1 == new_dataloaders['train'].batch_size, 'Mismatch between batch sizes.'\n","#             # write_results('{}'.format(xplus1), results_files['bse'])\n","#             write_csv([ctbs(l) for l in logits], results_files['end-samples'])\n","#             write_csv([w.item() for w in weights], results_files['end-alpha'])\n","#             sched_optimiser.param_groups[1]['params'] = bs_scheduler.alpha_params(reset=True)\n","            \n","#             ''' Run schedule \n","#             ''' \n","#             if bs_scheduling:\n","#               if not smith_heuristic and (epoch+1) in bs_milestone:\n","#                 idx = bs_milestone.index(epoch+1)\n","#                 old_batch_size = dataloaders['train'].batch_size\n","#                 new_batch_size = bs[idx]\n","#                 print('batch size changed from {} to {}.'.format(old_batch_size, new_batch_size))\n","#                 new_dataloaders = get_dataloaders(dataset, batch_size=new_batch_size)\n","                \n","#                 ''' bring phi samples close to new batch size\n","#                 '''\n","#                 warmup(bs_scheduler, sched_optimiser, new_dataloaders) \n","#                 print('done')\n","\n","#                 # ''' **********\n","#                 # #     new idea: restart agent after heuristic change (26/4/22)\n","#                 # #     ********** \n","#                 # # '''\n","#                 # # bs_scheduler = bsg(n_samples=n_samples,n_features=net.n_features, results_files=results_files).cuda()\n","#                 # print('NEW DYNAMICS...Arbiter has been reset.')\n","#                 # print('# parameters in Arbiter: {}'.format(sum(p.numel() for p in bs_scheduler.parameters())))\n","#                 # sched_optimiser = torch.optim.Adam([\n","#                 #                   {'params':bs_scheduler.parameters(),   'lr':phi_lr},\n","#                 #                   {'params':bs_scheduler.alpha_params(), 'lr':alpha_lr},\n","#                 #                   ], weight_decay=wd)\n","#                 # optimisers = {'net':net_optimiser, 'sched':sched_optimiser}\n","\n","#                 # ''' bring phi samples close to new batch size\n","#                 # '''\n","#                 # warmup(bs_scheduler, sched_optimiser, new_dataloaders)\n","#                 # print('done')\n","\n","\n","#               elif smith_heuristic and (epoch+1) in smith_milestones:\n","#                 ''' implement hybrid heuristic from Smith 2018\n","#                 '''\n","#                 # batch size: only change once at 60 epochs then constant.\n","#                 if (epoch+1) == smith_milestones[0]:\n","#                   old_batch_size = dataloaders['train'].batch_size # most recent batch size set by Arbiter\n","#                   # new_batch_size = xplus1 \n","#                   new_batch_size = 5*batch_size                    # scale original batch size by Smiths heuristic (x5)\n","#                   print('batch size changed from {} to {}.'.format(old_batch_size, new_batch_size))\n","#                   new_dataloaders = get_dataloaders(dataset, batch_size=new_batch_size)\n","\n","#                   ''' new idea (29/4)\n","#                       let arbiter solve for next batch size at 60th epoch, then remove arbiter entirely and let\n","#                       the rest of Smiths heuristic play out with that batch size.\n","#                   '''\n","#                   turn_off = True\n","#                   print('Arbiter turned off off. Returned to normal training under his predicted batch size of {}.'.format(dataloaders['train'].batch_size))\n","\n","#                   # ''' **********\n","#                   #     new idea: restart agent after heuristic change (26/4/22)\n","#                   #     ********** \n","#                   # '''\n","#                   # bs_scheduler = bsg(n_samples=n_samples,n_features=net.n_features, results_files=results_files).cuda()\n","#                   # print('NEW DYNAMICS...Arbiter has been reset.')\n","#                   # print('# parameters in Arbiter: {}'.format(sum(p.numel() for p in bs_scheduler.parameters())))\n","#                   # sched_optimiser = torch.optim.Adam([\n","#                   #                   {'params':bs_scheduler.parameters(),   'lr':phi_lr},\n","#                   #                   {'params':bs_scheduler.alpha_params(), 'lr':alpha_lr},\n","#                   #                   ], weight_decay=wd)\n","#                   # optimisers = {'net':net_optimiser, 'sched':sched_optimiser}\n","\n","#                   # ''' bring phi samples close to new batch size\n","#                   # '''\n","#                   # warmup(bs_scheduler, sched_optimiser, new_dataloaders)\n","#                   # print('done')\n","\n","#                   # ''' save all weights for quick restarts\n","#                   # '''\n","#                   # # inner system\n","#                   # torch.save(\n","#                   #     net.state_dict(), \n","#                   #     os.path.join(log_root, '{}_{}_valLoss={:4.4f}_valAcc={:4.4f}_epoch={}.pth.tar'.format(arch, dataset, val_stats['loss'], val_acc, epoch+1))\n","#                   #   )   \n","                  \n","#                   # # outer system\n","#                   # torch.save(\n","#                   #     bs_scheduler.state_dict(), \n","#                   #     os.path.join(log_root, 'scheduler_s={}_epoch={}.pth.tar'.format(new_batch_size, epoch+1))\n","#                   #     )\n","                  \n","#                 elif (epoch+1) in smith_milestones:\n","#                   # learning rate: only change at 120 and 160 (see milestones in MultiStepLR call)\n","#                   old_lr = net_optimiser.param_groups[0]['lr']\n","#                   print('old lr: {}'.format(old_lr))\n","#                   net_optimiser.param_groups[0]['lr'] *= 0.2\n","#                   # lr_scheduler.step()\n","#                   print('new lr: {}'.format(net_optimiser.param_groups[0]['lr']))\n","                  \n","#                   # ''' **********\n","#                   #     new idea: restart agent after heuristic change (26/4/22) \n","#                   #     **********\n","#                   # '''\n","#                   # bs_scheduler = bsg(n_samples=n_samples,n_features=net.n_features, results_files=results_files).cuda()\n","#                   # print('NEW DYNAMICS...Arbiter has been reset.')\n","#                   # print('# parameters in Arbiter: {}'.format(sum(p.numel() for p in bs_scheduler.parameters())))\n","#                   # sched_optimiser = torch.optim.Adam([\n","#                   #                   {'params':bs_scheduler.parameters(),   'lr':phi_lr},\n","#                   #                   {'params':bs_scheduler.alpha_params(), 'lr':alpha_lr},\n","#                   #                   ], weight_decay=wd)\n","#                   # optimisers = {'net':net_optimiser, 'sched':sched_optimiser} \n","                  \n","#                   # ''' bring phi samples close to new batch size\n","#                   # '''\n","#                   # warmup(bs_scheduler, sched_optimiser, new_dataloaders)\n","#                   # print('done')\n","\n","#         ''' print stats\n","#         '''\n","#         cur_lr = net_optimiser.param_groups[0]['lr']\n","#         cur_bs = dataloaders['train'].batch_size\n","#         print('end of epoch lr: {}'.format(cur_lr))\n","#         print('end of epoch BS: {}'.format(cur_bs))\n","#         print('          LR/BS: {}'.format(cur_lr/cur_bs))\n","#         write_results('{}'.format(cur_lr), results_files['lrep'])  \n","#         write_results('{}'.format(cur_bs), results_files['bse'])  \n","#         write_results('{}'.format(cur_lr/cur_bs), results_files['ratio'])  \n","       \n","#         if save_results:\n","#             write_results('{:.6f}'.format(train_stats['loss']), results_files['trl'])\n","#             write_results('{:.6f}'.format(train_acc), results_files['tra'])\n","#             write_results('{:.6f}'.format(val_stats['loss']), results_files['vl'])\n","#             write_results('{:.6f}'.format(val_acc), results_files['va'])\n","#             write_results('{:.6f}'.format(test_stats['loss']), results_files['tel'])\n","#             write_results('{:.6f}'.format(test_acc), results_files['tea'])\n","        \n","#         if val_acc > best_acc:\n","#             best_acc = val_acc\n","#             print('best val acc so far: {}'.format(best_acc))\n","#             if save_weights:\n","#               torch.save(\n","#                 net.state_dict(), \n","#                 os.path.join(model_dir, '{}_{}_valLoss={:4.4f}_valAcc={:4.4f}_epoch={}.pth.tar'.format(arch, dataset, val_stats['loss'], val_acc, epoch+1))\n","#               )   \n","               \n","# def train(net, bs_scheduler, dataloaders, criterion, optimisers, turn_off):\n","    \n","#     net.train()   \n","#     bs_scheduler.train()\n","#     net_optimiser = optimisers['net']\n","#     sched_optimiser = optimisers['sched']\n","#     correct = 0\n","#     loss_list = []\n","    \n","#     for i, (inputs, targets) in enumerate(dataloaders['train']):\n","\n","#         write_results('{}'.format(dataloaders['train'].batch_size), results_files['bsi'])\n","        \n","#         net_optimiser.zero_grad()\n","#         sched_optimiser.zero_grad()\n","#         custom_dataloader = dataloaders['custom'] \n","            \n","#         inputs, targets = inputs.cuda(), targets.cuda()\n","#         outputs = net(inputs)\n","#         loss = criterion(outputs, targets)\n","#         loss_list.append(loss.item())\n","#         correct += count_correct(outputs, targets)\n","#         loss.backward()\n","#         net_optimiser.step() \n","\n","#         if not turn_off:\n","#             val_inputs, _ = next(iter(dataloaders['val']))\n","#             logits, l_alpha, S_alpha, hparam = bs_scheduler(val_inputs.cuda())\n","#             new_data, new_targets = custom_dataloader(sample_size=S_alpha)\n","#             new_data, new_targets = new_data.cuda(), new_targets.cuda()\n","            \n","#             new_outputs = net(new_data, hparam=hparam)    \n","#             F = criterion(new_outputs, new_targets)\n","#             F.backward()\n","#             sched_optimiser.step()\n","#             write_results('{}'.format(F.item()), results_files['F'])             \n","        \n","#     if not turn_off:\n","#         return logits, {'correct': correct, 'loss': np.mean(loss_list)}\n","#     else:\n","#         return {'correct': correct, 'loss': np.mean(loss_list)}\n","\n","\n","\n","# def validation(net, dataloader, criterion):\n","#     net.eval()\n","#     correct = 0\n","#     loss_list = []\n","#     with torch.no_grad():\n","#         for i, (data, target) in enumerate(dataloader):\n","#             data, target = data.cuda(), target.cuda()\n","#             output = net(data) \n","#             loss = criterion(output, target)\n","#             loss_list.append(loss.item())\n","#             correct += count_correct(output, target)\n","#     return {'correct': correct, 'loss': np.mean(loss_list)}\n","\n","\n","# def test(net, dataloader, criterion):\n","#     net.eval()\n","#     correct = 0\n","#     loss_list = []\n","#     with torch.no_grad():\n","#         for i, (data, target) in enumerate(dataloader):\n","#             data, target = data.cuda(), target.cuda()\n","#             output = net(data) \n","#             loss = criterion(output, target)\n","#             loss_list.append(loss.item())\n","#             correct += count_correct(output, target)\n","#     return {'correct': correct, 'loss': np.mean(loss_list)}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7NmvrS2xFvM8"},"source":["# setup"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Dxjx7uhrF8KM","executionInfo":{"status":"ok","timestamp":1660132484524,"user_tz":-60,"elapsed":370,"user":{"displayName":"Calum MacLellan","userId":"18398237058780708126"}}},"outputs":[],"source":["arch='vgg'\n","dataset='cifar_10'\n","num_epochs=10\n","batch_size=128\n","optim_type='sgd'\n","lr_scheduler_type='off'\n","hd_hyperlr=1e-3\n","lr=0.1\n","lr_decay=0.2\n","milestones=[50, 75]\n","t_0=10\n","t_mult=2\n","momentum=0.9\n","wd=5e-4\n","alpha_lr=1e-6   # use 1e-4 for next bs scheduling exp (too committed!: do 1e-5 again); 1e-5 for next lr exp\n","phi_lr=1e-6     # use 2.5e-7 r next bs scheduling exp (make 1e-6); 1e-5 for next lr exp\n","seed=20\n","n_samples = 4\n","s_min=8\n","s_max=1000\n","sched_int=1\n","train_percent=0.8\n","\n","bs = [128, 256, 512]\n","bs_milestone = [25, 50, 100]\n","smith_milestones = [60,120,160] # hybrid heuristic from Smith 2018. \n","smith_heuristic = False \n","bs_scheduling  =  False \n","\n","save_results=True\n","save_weights=False\n","save_arbiter = True\n","use_warm=False\n","root=root_path\n","\n","arb_dir=os.path.join(root,'experiments/arb_ckpts')\n","model_dir=os.path.join(root,'experiments/net_ckpts')\n","log_root=os.path.join(root, 'experiments/logs')\n","\n","# model_dir=os.path.join(root,'appendix/ablation-study/learning-rates/a=1e-5,p=1e-3/checkpoints')\n","# model_dir=os.path.join(root,'experiments/heuristics/smith-hybrid/colab-scripts/arbiter/B-after-60-epochs/checkpoints')\n","# model_dir=os.path.join(root,'experiments/heuristics/smith-hybrid/colab-scripts/arbiter/checkpoints')\n","# log_root=os.path.join(root, 'experiments/heuristics/smith-hybrid/colab-scripts/arbiter/logs')\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"3sDqcifNFw-s","executionInfo":{"status":"ok","timestamp":1660132488808,"user_tz":-60,"elapsed":1916,"user":{"displayName":"Calum MacLellan","userId":"18398237058780708126"}}},"outputs":[],"source":["if save_arbiter and not os.path.exists(arb_dir): os.mkdir(arb_dir)\n","if save_weights and not os.path.exists(model_dir): os.mkdir(model_dir)\n","if save_results and not os.path.exists(log_root): os.mkdir(log_root)\n","    \n","   \n","settings_txt = open(os.path.join(log_root, '_settings.txt'), 'w')\n","write_results('Experiment settings:', settings_txt)\n","write_results('\\tarch:           {}'.format(arch), settings_txt)\n","write_results('\\tdataset:        {}'.format(dataset), settings_txt)\n","write_results('\\tnum_epochs:     {}'.format(num_epochs),settings_txt)\n","write_results('\\tbatch size:     {}'.format(batch_size),settings_txt)\n","write_results('\\toptimiser:      {}'.format(optim_type),settings_txt)\n","write_results('\\tlr scheduler:   {}'.format(lr_scheduler_type),settings_txt)\n","write_results('\\thd hyper lr:    {}'.format(hd_hyperlr),settings_txt)\n","write_results('\\tlr:             {}'.format(lr),settings_txt)\n","write_results('\\tlr decay:       {}'.format(lr_decay),settings_txt)\n","write_results('\\tmilestones:     {}'.format(milestones),settings_txt)\n","write_results('\\tt0:             {}'.format(t_0),settings_txt)\n","write_results('\\tt_mult:         {}'.format(t_mult),settings_txt)\n","write_results('\\tmomentum:       {}'.format(momentum),settings_txt)\n","write_results('\\tweight decay:   {}'.format(wd),settings_txt)\n","write_results('\\tphi lr:         {}'.format(phi_lr),settings_txt)\n","write_results('\\talpha lr:       {}'.format(alpha_lr),settings_txt)\n","write_results('\\tn_samples:      {}'.format(n_samples),settings_txt)\n","write_results('\\ts_min:          {}'.format(s_min),settings_txt)\n","write_results('\\ts_max:          {}'.format(s_max),settings_txt)\n","write_results('\\tsched_int:      {}'.format(sched_int),settings_txt)\n","write_results('\\ttrain_percent:  {}'.format(train_percent),settings_txt)\n","write_results('\\tsave_results:   {}'.format(save_results),settings_txt)\n","write_results('\\tsave_weights:   {}'.format(save_weights),settings_txt)\n","write_results('\\tuse_warm:       {}'.format(use_warm),settings_txt)\n","\n","\n","train_loss_file = open(os.path.join(log_root, 'train_loss.txt'), 'w')\n","train_acc_file = open(os.path.join(log_root, 'train_acc.txt'), 'w')\n","val_loss_file = open(os.path.join(log_root, 'val_loss.txt'), 'w')\n","val_acc_file = open(os.path.join(log_root, 'val_acc.txt'), 'w')\n","test_loss_file = open(os.path.join(log_root, 'test_loss.txt'), 'w')\n","test_acc_file = open(os.path.join(log_root, 'test_acc.txt'), 'w')\n","\n","logits_file = open(os.path.join(log_root, 'logits.csv'), 'w')\n","bs_samples = open(os.path.join(log_root, 'bs_samples.csv'), 'w')\n","l_alpha_file = open(os.path.join(log_root, 'l_alpha.txt'), 'w')\n","alphas_file = open(os.path.join(log_root, 'alphas.csv'),  'w')\n","F_file = open(os.path.join(log_root, 'F.txt'), 'w')\n","\n","s_alpha_file = open(os.path.join(log_root, 's_alpha.txt'), 'w')\n","llu = open(os.path.join(log_root, 'bse.txt'), 'w')\n","lpo = open(os.path.join(log_root, 'bsi.txt'), 'w')\n","lr_iter_txt    = open(os.path.join(log_root, 'lr_iter.txt'),    'w')\n","lr_epoch_txt   = open(os.path.join(log_root, 'lr_epoch.txt'),   'w')  \n","hypergrad_file = open(os.path.join(log_root, 'h.txt'), 'w')\n","ratio_txt   = open(os.path.join(log_root, 'ratio.txt'),   'w')  \n","best_sample_txt   = open(os.path.join(log_root, 'best_sample.txt'),   'w')  \n","\n","epochTime_file = open(os.path.join(log_root, 'epoch_times.txt'), 'w')\n","end_of_epoch_alphas = open(os.path.join(log_root, 'epoch_alphas.csv'), 'w')\n","end_of_epoch_samples = open(os.path.join(log_root, 'epoch_samples.csv'), 'w')\n","\n","results_files = {'sett':settings_txt,\n","                'trl':train_loss_file, 'tra':train_acc_file,\n","                'vl':val_loss_file,'va':val_acc_file,\n","                'tel':test_loss_file,'tea':test_acc_file,\n","                'l':logits_file, 'samples':bs_samples, 'la':l_alpha_file, 'best':best_sample_txt,\n","                'alpha':alphas_file, 'F':F_file, \n","                'sa':s_alpha_file, 'bse':llu, 'bsi':lpo,\n","                'times':epochTime_file, 'end-alpha':end_of_epoch_alphas, 'end-samples':end_of_epoch_samples,\n","                'lrep':lr_epoch_txt, 'lrit':lr_iter_txt, 'h':hypergrad_file, 'ratio':ratio_txt,\n","}\n"]},{"cell_type":"markdown","source":["# run "],"metadata":{"id":"IH9TV42v0kLp"}},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":416},"id":"Py4tBVj5F99H","outputId":"6b1d2316-00d8-407c-b2c3-08fe46c934c2","executionInfo":{"status":"error","timestamp":1660132905239,"user_tz":-60,"elapsed":1805,"user":{"displayName":"Calum MacLellan","userId":"18398237058780708126"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","\n","lr decay off.\n","# parameters in Arbiter: 39768\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-fdbf9f13238e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-37-1f464238affa>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(results_files)\u001b[0m\n\u001b[1;32m     50\u001b[0m                                     \u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbs_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mphi_lr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                                     \u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbs_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0malpha_lr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                                     ], weight_decay=wd)\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0moptimisers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'net'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnet_optimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sched'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msched_optimiser\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable)\u001b[0m\n\u001b[1;32m     88\u001b[0m                         \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamsgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                         maximize=maximize, foreach=foreach, capturable=capturable)\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36madd_param_group\u001b[0;34m(self, param_group)\u001b[0m\n\u001b[1;32m    283\u001b[0m                             'the ordering of tensors in sets will change between runs. Please use a list instead.')\n\u001b[1;32m    284\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'Linear' object is not iterable"]}],"source":["main(results_files)"]},{"cell_type":"markdown","metadata":{"id":"ag4vOgihsrMw"},"source":["# Examine outputs "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1906,"status":"ok","timestamp":1657618312503,"user":{"displayName":"Calum MacLellan","userId":"18398237058780708126"},"user_tz":-60},"id":"w7V3anPCs2ua","outputId":"5d410e49-2b8a-4432-a714-4aefe2f36031"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":51}],"source":["\n","arb_dir=os.path.join(root,'experiments/analysis')\n","\n","dataloaders = get_dataloaders(dataset, batch_size=10)\n","net = get_network(arch, dataset).cuda()\n","arbiter = bsg(n_samples=4, n_features=net.n_features, results_files=None).cuda()\n","# arbiter.load_state_dict(torch.load(os.path.join(arb_dir, 'arbiter_bestSample=145_epoch=1.pth.tar')))\n","# arbiter.load_state_dict(torch.load(os.path.join(arb_dir, 'input=data/arb_ckpts/arbiter_bestSample=405_epoch=97.pth.tar')))\n","arbiter.load_state_dict(torch.load(os.path.join(arb_dir, 'input=noise/arb_ckpts/arbiter_bestSample=247_epoch=15.pth.tar')))\n","# arbiter.eval()"]},{"cell_type":"code","source":["\n","val_inputs, _ = next(iter(dataloaders['val']))\n","samples, logit_S, S, hparam = arbiter(val_inputs.cuda(), warmup=True)\n","print([ctbs(l) for l in samples], S)\n","\n","\n","# new_data, new_targets = custom_dataloader(sample_size=S)\n","# new_data, new_targets = new_data.cuda(), new_targets.cuda()\n","\n","# new_outputs = net(new_data, hparam=hparam)    \n","# F = criterion(new_outputs, new_targets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KauXlyFZp5rq","executionInfo":{"status":"ok","timestamp":1657618317717,"user_tz":-60,"elapsed":239,"user":{"displayName":"Calum MacLellan","userId":"18398237058780708126"}},"outputId":"fda209cf-ab4f-458e-a778-858c887a8b36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[138, 150, 156, 170] 153\n"]}]},{"cell_type":"markdown","source":["# embeddings"],"metadata":{"id":"E99WHn7ZQ5Ht"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","embedding = nn.Embedding(10, 3)\n","input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n","embedding(input)\n","\n","# tensor([[[-0.0251, -1.6902,  0.7172],\n","#          [-0.6431,  0.0748,  0.6969],\n","#          [ 1.4970,  1.3448, -0.9685],\n","#          [-0.3677, -2.7265, -0.1685]],\n","\n","#         [[ 1.4970,  1.3448, -0.9685],\n","#          [ 0.4362, -0.4004,  0.9400],\n","#          [-0.6431,  0.0748,  0.6969],\n","#          [ 0.9124, -2.3616,  1.1151]]])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GGI5COYnQ4qQ","executionInfo":{"status":"ok","timestamp":1657576868169,"user_tz":-60,"elapsed":208,"user":{"displayName":"Calum MacLellan","userId":"18398237058780708126"}},"outputId":"ec5ac6d8-1109-4469-e9c2-c0695db1f755"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 1.4791, -0.3318, -0.5765],\n","         [-0.0376,  1.0059, -0.8638],\n","         [-0.1467, -0.5646, -0.3802],\n","         [ 0.3114, -1.0607, -0.7022]],\n","\n","        [[-0.1467, -0.5646, -0.3802],\n","         [-0.7761, -0.1339,  0.3454],\n","         [-0.0376,  1.0059, -0.8638],\n","         [-1.3845, -3.3406, -0.8055]]], grad_fn=<EmbeddingBackward0>)"]},"metadata":{},"execution_count":11}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"arbiter.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}